% Meta-programming in Nemerle
%
\documentclass{llncs}
%
\newcommand{\net}[0]{{\tt .NET}}
\newcommand{\netf}[0]{{\tt .NET} Framework}
\newcommand{\nem}[0]{Nemerle}
\newcommand{\cs}[0]{C\#}
\newcommand{\oo}[0]{object-oriented}
\newcommand{\kw}[1]{{\tt \bf #1}}

\newcommand{\infrule}[3]{
  \displaystyle
  \frac{#1}{#2}
  \, #3
}
\newcommand{\nf}[2]{#1 \rightarrow #2}
\newcommand{\colr}[2]{#1 \Rightarrow^{u}_{c} \, #2}

%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
%
\title{Meta-programming in \nem}
%
\titlerunning{Meta-programming in Nemerle}  
%
\author{Kamil Skalski \and Michal Moskal \and Pawel Olszta}
%
\authorrunning{Kamil Skalski et al.} 
%
%
\institute{University of Wroclaw, Poland \\
           \texttt{http://www.nemerle.org/}}
%
\maketitle              % typeset the title of the contribution
%
\begin{abstract}
We present the design of a meta-programming system embedded into \nem\,,
a new functional language for the \net\ platform. The system enables
compile-time operations -- generation, transformation and automated 
analysis of programs by means of hygienic code quotation, syntax 
extensions, operating on the code like on any other datatype (e.g. listing, 
adding or changing members of class definition), performing partial 
typing of a program syntax tree (compiler internal typing procedures 
are executed by a macro code) and interoperability with the compilation 
process. All these operations can be fully parametrized with any 
external data (like a database, a file or a web page).

Our system is a convenient tool for Aspects Oriented Programming with 
the ability to operate on datatypes, traverse the program code and perform 
various algorithmic operations on its content.
\end{abstract}

\section{Introduction}
The idea of compile-time meta-programming has been studied for quite a long time.
It was incorporated into several languages, like Lisp macros \cite{Lisp:Macros}, 
Scheme hygienic macros \cite{Scheme:Macros}, C preprocessor-based macros, 
C++ template system and finally 
Haskell Template Meta-programming \cite{Haskell:Meta}. 
They vary in their capabilities and ease of use, but generally imply computations 
during compilation of the program and generating code from some definitions.

During this process programs are treated as \emph{object programs}, which are 
data supplied to \emph{meta-programs}. They can be then arbitrarily transformed or 
analyzed and the final result is compiled just like a regular program. These
operations may be repeated or take place in stages. In the latter case the 
generated programs can generate other programs and so on.

\emph{Meta-language} is a language for programming such operations. It 
usually has its own syntax for describing various constructs of the object language.
For example, in our system, \verb,<[ 1 + f (2 * x) ]>, denotes the syntax 
tree of expression \verb,1 + f (2 * x),. This idea is called \emph{quasi-quotation}. 
The prefix \emph{quasi} comes from the possibility of inserting values 
of meta-language expressions into the quoted context -- if \verb,g(y), is such 
an expression, we can write \verb.<[ 1 + $(g (y)) ]>., %$ 
which describes a syntax tree, whose second part is replaced by the result of 
evaluation of \verb,g(y),.

\subsection{Our contribution}
While we introduce several new ideas, the most important thing about
our approach, is the unique combination of powerful meta-programming
system together with the language, that posses industrial strength
object-oriented and imperative capabilities.

This paper is a study of possible introduction of meta-programming
techniques into an industrial environment. We are therefore focused on
making our system easy to use for programmers both writing and using
the macros.
% chyba juz wystarczajaco marketingowo?

Key features of our approach are:

\begin{itemize}
  \item We develop a uniform, hygiene preserving and simple quasi-quotation system, 
    which does not require learning of internal compiler data structures to generate 
    and transform quite complicated object programs. It also provides an easy way to 
    write variable argument constructs (like tuples or function calls with an arbitrary 
    amount of parameters).
  \item Using macros is transparent from the user point of view -- the meta-program and 
    common function calls are indistinguishable, so the user can use the most complex
    macros prepared by others without even knowing the idea of meta-programming.
  \item Flexible definition of syntax extensions allows even more straightforward 
    embedding of macros into the language without interfering with compiler internals.
  \item Our system can be used to transform or generate practically any fragment
    of a program, which, composed with \net\ \oo\ structure, provides a powerful tool for
    software engineering methodologies like aspects-oriented programming.
  \item We allow macros to type fragments of the code, which they operate on, during
    their execution. This allows to parameterize them not only with the syntax of provided
    expressions, but also with the entire context of the program and types of those 
    expressions.
  \item The separate compilation of macro definitions provides the clean and manageable 
    stage separation.
\end{itemize}

\subsection{Characteristics of \nem\ meta-system}
Our meta-system has both \emph{program generation} and \emph{analysis}
capabilities \cite{Meta:Accomplishments}. It can easily walk through the abstract 
syntax tree of the object program and gather information about it as well 
as change it (often using gathered data).

The system is designed mainly for operating on object programs at the compile-time.
However, using features of \net\ and its dynamic code loading abilities, it is
also possible to execute macros during run-time. 

The meta-language is \emph{homogeneous}, which means that it is the same as the 
object language. We can use common \nem\ functions within macros and the syntax 
of generated programs is no different than the one used to write macros.

The quasi-quotation provides a clear separation of the object program from 
the meta-language. This manual annotation provides a way to separate the stages
of execution in a well understood fashion. It is also semantically clear, which 
part of the code is generated and which is generating. The symbols from the 
object-code are alpha-renamed so that they do not interfere with the external code. 

\section{First examples}
Suppose we want to add some new syntax to our language, like the \kw{for} loop.
We could embed it into the compiler, but it is a rather hard and inelegant way -- such 
addition is quite short and it should not involve much effort to complete. 
Here a macro can be used.

\begin{verbatim}
macro for (init, cond, change, body) 
{
  <[ 
    $init;
    def loop () {
      if ($cond) { $body; $change; loop() } 
      else () 
    }; 
    loop ()
  ]>
}
\end{verbatim}

This code creates a special meta-function, which is executed at the compile-time
in every place where its original call is placed. Its result is then inserted 
into the program. Always when something like

\begin{verbatim}
  for (i <- 0, i < n, i <- i + 2, a[i] <- i)
\end{verbatim}

\noindent
is written, the appropriate code is created according to the \kw{for} macro and 
replaces original call.

The macros may instruct the compiler to extend the language syntax -- for example 
a macro for the \kw{for} loop with a C-like syntax can be defined. Writing

\begin{verbatim}
  macro for (init, cond, change, body) 
  syntax ("for", "(", init, cond, change, ")", body) { ... }
\end{verbatim}

\noindent
would add a new rule to the parser, which allows using

\begin{verbatim}
  for (i <- 0; i < n; i <- i + 2) a[i] <- i
\end{verbatim}

\noindent
instead of the call mentioned above.

\subsection{Compiling sublanguages from strings}
Macros are very useful for the initial checking and processing of the code
written as strings in a program. This relates to many simple languages,
like \kw{printf} formatting string, regular expressions or even SQL, which are 
often used directly inside the program. 

Let us consider a common situation, when we want to parameterize an SQL query
with some values from our program. Most database providers in 
\netf\ allow us to write commands with parameters, but neither their syntax 
is checked during the compilation, nor the consistency of the SQL data types 
with the program is controlled.

With a well written macro, we could write
\begin{verbatim}
  sql_loop (conn, "SELECT salary, LOWER (name) AS lname"
                  "  FROM employees"
                  "  WHERE salary > $min_salary") {
    print ("$lname : $salary\n")
  }
\end{verbatim} %$

to obtain the syntax and type-checked SQL query and the following code
\begin{verbatim}
  def cmd = SqlCommand ("SELECT salary, LOWER (name)"
                        "  FROM employees"
                        "  WHERE salary > @parm1", conn);

  (cmd.Parameters.Add (SqlParameter ("@parm1", DbType.Int32)))
    .Value <- min_salary;
  def r = cmd.ExecuteReader ();
  while (r.Read ()) {
    def salary = r.GetInt32 (0);
    def lname = r.GetString (1);
    print ("$lname : $salary\n")
  }
\end{verbatim}

In fact the \verb,print, function here is another macro, that generates 
the following code, basing on the given string parameter:

\begin{verbatim}
  System.Console.Write (lname.ToString ());
  System.Console.Write (" : ");
  System.Console.Write (salary.ToString ());
  System.Console.Write ("\n")
\end{verbatim}

\section{Variable amount of arguments}
The quotation provides full freedom in constructing any kind of expression.
For example, we can decompose a tuple of any size and print its elements.

\begin{verbatim}
  macro PrintTuple (tup, size : int)
  {
    def symbols = array (size);
    mutable pvars <- [];
    for (mutable i <- size - 1; i >= 0; i <- i - 1) {
      symbols[i] <- NewSymbol ();
      pvars <- <[ pattern: $(symbols[i] : var) ]> :: pvars;
    };
    mutable exps <- [];
    for (mutable i <- size - 1; i >= 0; i <- i - 1)
      exps <- <[ WriteLine ($(symbols[i] : var)) ]> :: exps;

    exps <- <[ def (.. $pvars) = $tup ]> :: exps;
    <[ {.. $exps } ]>
  }
\end{verbatim} %$

Note that here we need a number describing the size of the tuple. We show later, 
how to obtain the type of the given expression within the macro. This, for 
example, allows to compute the size of the tuple described by the \verb,tup, variable.

\section{Pattern matching on programs}
The quotation can be used to analyze the program structure as easily as generate
it. Standard mechanisms of the language, like the pattern matching, fit perfect
for such a purpose. 

Suppose we want to check the code for occurrences of the following buggy
fragments:
\begin{verbatim}
 if ( foo != null && foo.bar || foo.blah ) foo.blah
\end{verbatim}

We write a macro, which inspects the given program to signal such constructs
(this is a simplified version, that will warn more than actually needed).
\begin{verbatim}
macro analyze (exp) : void 
{
  def traverse (ex, checked : Set<Name>) : Set<Name> {
    match (ex) {
      | <[ $(name : var) != null ]> => 
        checked.Add (name)
      | <[ $(name : var).$(_) ]> => 
        if (!checked.Contains (name)) 
          throw MaybeBug() 
        else 
          checked
      | <[ $e1 && $e2 ]> => 
        traverse (e2, traverse (e1, checked))
      | <[ $e1 || $e2 ]> => 
        def chck1 = traverse (e1, checked);
        def chck2 = traverse (e2, checked);
        Intersect (chck1, chck2)
      | <[ if ($cond) $e1 else $e2 ]> =>
        traverse (e1, traverse (cond, checked));
        traverse (e2, checked)
      | _ => Macros.ApplyExpr (exp, fun (x) { 
          traverse (x, checked) 
        })
    }
  };
  Macros.ApplyExpr (exp, fun (x) { traverse (x, Set ()) })
}
\end{verbatim} %$

\section{Macros on declarations} \label{Declarations}
Macros can operate not only on expressions, patterns, types, but also on
any part of language, like classes, interfaces, other type declarations,
methods, etc. The syntax for those operations is quite different. Again, we
treat language constructs as data objects, which can be transformed, but
this is not done entirely with quotations. We use a special API, designed 
basing on \verb,System.Reflection,, which is used in \net\ for dynamic
generation of assemblies. \nem\ type system and operations we are performing
on them are not fully compatible with the interface of \verb,Reflection, 
(we cannot directly derive from its classes), but are quite similar.

With such a tool we can analyze, generate or change any declaration
in the program. For example, dump definition of each data structure 
to an XML file, create serialization methods or automatically
generate fields or methods from an external description.

Such macros are not called like ordinary functions, but are added as
attributes in front of the declaration, similarly as \cs\ attributes.

\begin{verbatim}
  [SerializeBinary ()] 
  public module Company {
    [ToXML ("Company.xml")] 
    public class Employee {
      ...
    }

    [FromXML ("Product.xml"), Comparable ()] 
    public class Product { }
  }
\end{verbatim}

\subsection{Transforming types}
Macros that operate on declarations change them in the imperative fashion.
Their first parameter is always an object representing the given 
declaration. 

\begin{verbatim}
  macro ToXML (c : Type, file : string) { 
\end{verbatim}

We can easily list the data contained in the provided object, like fields 
or methods of a class and add a new method using their names.

\begin{verbatim}
  def fields = c.GetFields ();
  def methods = c.GetMethods ();
  def method_builder = 
    c.DefineMethod ("ToXML", MethodAttributes.Public, 
                    <[ type: void ]>, []);
  def list_fields = 
    List.Map (fields, fun (x) { <[ 
      xml_writer.WriteAttributeString ($(x.Name.GetId () : string), 
                                       $(x.Name : var).ToString ()) 
    ]> }
  );
  method_builder.SetBody (<[
    def xml_writer = XmlTextWriter ($(file : string), null);
    { ..$list_fields };
    xml_writer.Close ()
  ]>)
\end{verbatim}

With the macro above (perhaps modified to do some additional formatting)
we can generate serialization methods for any class simply by adding 
the \verb,[ToXML ("file.xml")], attribute.

\section{Aspects-oriented programming}
AOP has been proposed as a technique for separating of different 
\emph{concerns} in software. It is often a problem, when they crosscut 
natural modularity of the rest of implementation and must be written 
together in code, leading to tangled code. 

\subsection{Join points and pointcuts}
\emph{Join points} are well-defined points in the execution of a program.
In contrary to many other systems, which define fixed \emph{join points}
in the code (like AspectJ \cite{AspectJ}), macros allow to place the user code in 
arbitrary parts of program. One can write a macro traversing all the classes
and adding some behavior to their bodies. 

A \emph{pointcut} picks out join points. In Nemerle they are not
restricted by any design, but the user can write a program that selects them.
Particularly we can write one which enables the same join points and pointcuts 
as those well estabilished in AOP world.

\subsection{Advices}
\emph{Advice} is code that executes at each join point picked out by a pointcut.
It's strightforward for macro to add \emph{advices} to code, by simply transforming
and combining it from given subprograms.

Consider the following code, adapted from the AspectJ tutorial: 

\begin{verbatim}
  after(Object o) throwing (Error e): publicInterface(o) {
    log.Write (o, e);
  }
\end{verbatim}

Given some traversal strategy, one can detect places where exceptions of some
type are thrown and add their logging..

\begin{verbatim}
  match (e) {
    | <[ throw $e ]> when IsAppropriateType (e) =>
      <[ log.Write ($o, $e); throw $e ]>
    | _ => e
  }
\end{verbatim}

\subsection{User interface to AOP features}
In order to make aspects oriented paradigm production-ready in Nemerle, we have
to develop easy user interface to . In general programmers would not use all power
of macro transformations in every day development. It is better to adapt existing
interfaces and progressively add new power of expresiveness to systems like AspectJ.

We think that it should be possible to implement most Aspects-oriented
and Adaptive-programming system designs with more or less complex 
macros. This field is our future research direction and we will present
more details after finishing the implementation of all the necessary features.

\section{Details of our design}
In this section a more formal definition of the macro and our meta-system is provided.

A macro is a top-level function prefixed with the \kw{macro} keyword, 
which may have access modifiers like other methods (\verb,public,, \verb,private, etc.) 
and resides in \net/\nem\ namespace. It is used within the code like
any other method, but it is treated in a special way by the compiler.

Its formal parameter types are restricted to the set of \nem\
grammar elements (including simple literal types). Parameters of 
the macro are always passed as their syntax trees, which for some
types are partially decomposed. For example, literal types appear
within the macro as real values (\verb,int,, \verb,string,, etc.),
but they are passed
as syntax trees, so they must be given as constant literals (it is
obvious since these values must be known at the compile-time).

\subsection{Macro as a function}
A macro cannot be called recursively or passed as a first-class citizen
(although it can generate the code, which contains the calls of this macro).
Still, it can use any function from the rest of the program in a standard ML
fashion, so we consider this as a minor disadvantage. If complex 
computations on syntax trees are necessary, one must simply put them 
into some arbitrary functions and run the entire computation from within 
the macro. Such design allows to define easily which functions are run 
at the compile-time without requiring any special annotations at their use site.

\subsection{Names binding in quotation}
A very important property of the meta-system is called \emph{hygiene} 
and relates to the problem with names capture in Lisp macros, resolved 
later in Scheme. It specifies that variables introduced by a macro 
may not bind to variables used in the code passed to this macro. Particularly 
variables with the same names, but coming from different contexts, should be 
automatically distinguished and renamed.

Consider the following example:

\begin{verbatim}
  macro identity (e) { <[ def f (x) { x }; f($e) ]> }
\end{verbatim} %$

Calling it with \verb,identity (f(1)), might generate a confusing code like

\begin{verbatim}
  def f (x) { x }; f (f (1))
\end{verbatim}

To prevent names capture, all macro-generated variables should be renamed 
to their unique counterparts, like in

\begin{verbatim}
  def f_42 (x_43) { x_43 }; f_42 (f (1))
\end{verbatim}

In general, names in the generated code bind to definitions visible within 
their scope. The binding is done after all transformations during the execution
of the macro are finished. This means that a variable used in a quotation 
may not necessarily refer to the definition visible directly in the place 
where it is written. Everything depends on where it occurs in the finally 
generated code. Consider the following example:

\begin{verbatim}
  macro foo (body) {
    def d1 = <[ def mail = login + "@nemerle.org" ]>;
    def d2 = <[ def login = $(developer : string) ]>
    <[ $d2;
       $d1;
       def msg = MailMessage ();
       msg.To <- mail;
       msg.Body <- $body;
       SmtpMail.Send (msg)
    ]>
  }
\end{verbatim}

As macros might get large and complex it is frequently very useful to compute
parts of the expression independently and then compose the final code from them.
Still, names in such macro are alpha-renamed so that they do not capture any 
external definitions. The renaming is defined as putting names created 
in the single macro execution into the same ``namespace'', which is
mutually exclusive with all other ``namespaces'' and the top-level code.
This is exactly the hygiene rule -- neither the macro can capture names
used in the macro-use place nor it can define anything colliding with the 
external code. 

This is an opposite approach to Template Haskell \cite{Haskell:Meta}, 
where the lexical scoping means binding variables from the object code immediately 
to definitions visible at the construction site of the quotation.
We find our approach more flexible, as we can transform the code with much
more freedom, while still keeping the system hygienic. This is of course just 
a design decision, naturally associated with certain costs. Sometimes it is not 
obvious to recreate bindings simply by looking on the code, but here we assume 
that a programmer of a macro knows the structure of the code to be generated. 
We also lose the ability to detect some errors earlier, but as they are always 
detected during the compilation of the generated code, we believe it is 
a minor disadvantage. 

One can think, that putting all identifiers from entire macro invocation into
a single ``namespace'' is not a good idea. Especially when we use some general 
purpose code generating function from library, which should generate only its 
own unique names. To obtain such independent, hygienic functions we write

\begin{verbatim}
[Hygienic] f (x : Expr) : Expr { ... }
\end{verbatim}

The \verb,[Hygienic], attribute is a simple macro, which transforms \verb,f, 
to enable its own context during execution. This way function receives the
same semantics as macro with regards to hygiene. We consider this behavior
not good by default, because code is often generated by some tool functions,
especially defined locally in macro and they should not change their context.

\subsection{Breaking hygiene}
Sometimes it is useful to share some names between few macro executions.
It can be done safely by generating a unique identifier independent of macro
executions. We support it by function \verb,NewSymbol (), whose return value
can be stored in a variable, providing the hygiene preserving solution.

There are also situations, where we know the exact name of the variable used in 
the code passed to the macro. If we wanted to define a name referring to it, we 
would have to change the scope to our macro use site. As it breaks hygiene, 
it should be done in a controlled manner. Consider a macro introducing \kw{using} 
keyword (C\# keyword, simplified for the purpose of this paper):

\begin{verbatim}
macro using (name : string, val, body) {
  def v = Macros.UseSiteSymbol (name);
  <[ 
    def $(v : var) = $val;
    try $body finally $v.Dispose ()
  ]>
}
\end{verbatim}

It should define a symbol binding to variables of the same name in \verb,body,.
But if it contained some other external code, like in:

\begin{verbatim}
macro bar(ext) { 
  <[ using ("x", Foo (), { $ext; x.Compute () }) ]> 
}
\end{verbatim} %$

\noindent
some inadvertent capture of variables in \verb,ext, might happen if 
\verb,x, was just a plain dynamically scoped variable.

Although it is not recommended, also nonhigienic symbols can be created, by
\verb,$(x : dyn),, where \verb,x, is of type \verb,string,. %$
They are bound to the nearest definition with the same name appearing in the
generated code, regardless of the context it comes from.

\subsection{Global symbols}
The object code often refers to variables, types or constructors imported
from other modules (e.g. \net\ standard library or symbols defined in 
the namespace where the macro resides). In normal code we can omit the prefix 
of the full name, by including \kw{using} keyword, which imports symbols from
given namespace. Unfortunately this feature used in the object code like

\begin{verbatim}
  using System.Text.RegularExpressions;

  public module Finder 
  {
    public static current : string;

    macro finddigit (x : string)
    {
      <[ 
        def numreg = Regex (@"\d+-\d+");
        def m = numreg.Match (current + x);
        m.Success ();
      ]>
    }
  }
\end{verbatim} %$

\noindent
brings some dependency on currently imported namespaces. We would like the
generated code to behave alike no matter where it used, thus the
\verb,Regex, constructor and the \verb,current, variable should be expanded to
their full name -- \verb,System.Text.RegularExpressions.Regex,
and \verb,Finder.current,, respectively. This operation is automatically
done by the quotation system. When a symbol is not defined locally (and with 
the same context as described in the previous section), its binding is 
looked for in global symbols imported at the quotation definition site.

Note that there is no possibility to override security permissions this
way. Access rights from the lexical scope of the macro are not exported to 
the place where the generated code is used. \net\ policies do not allow this, 
thus the programmer must not generate a code breaking the static security.

\subsection{Accessing compiler internals}
It is vital for meta-functions to be able to use all benefits they have
from running at the compile-time. They can retrieve information from the
compiler, use its methods to access, analyze and change data stored
in its internal structures. 

\subsubsection{Retrieving declarations}
For example, we can ask the compiler to return the \emph{type declaration} 
of a given \emph{type}. It will be available as the syntax tree, just like 
as we put a special macro attribute (Section \ref{Declarations}) before that declaration.
Certainly, such a declaration must not be an external type 
and has to be available within the compiled program as a source code.

\begin{verbatim}
  def decl = Macros.GetType (<[ type: Person ]>);
  xmlize (decl);    // we can use macros for declarations
\end{verbatim}

\section{Typing during execution of macro}
Some more advanced techniques are also possible. They involve a closer
interaction with the compiler, using its methods and data structures
or even interweaving with internal compilation stages.

For example, we can ask the compiler to type some of object programs,
which are passed to the macro, retrieve and compare their types, etc.
This means that we can plug between many important actions performed by 
the compiler, adding our own code there. It might be just a nicer 
bug reporting (especially for macros defining complex syntax extensions), 
making code generation dependent on input program types or improving
code analysis with additional information.

\subsection{Example}
Let us consider the following translation of the \kw{if} condition to the standard
ML matching:

\begin{verbatim}
  macro @if (cond, e1, e2)
  syntax ("if", "(", cond, ")", e1, "else", e1) 
  {
    <[ 
      match ($cond) {
        | true => $e1
        | false => $e2
      }       
    ]>
  }
\end{verbatim} % $

When \verb,if ("bar") true else false, was written, the compiler would comply
that type of matched expression is not of type \verb,bool,. Such an error 
message could be very confusing, because the programmer may not know, that his 
\verb,if, statement is being transformed to the \verb,match, statement. Thus, 
we would like to check such errors during the execution of the macro, so we can
generate more verbose message.

\subsection{Usage}
Instead of directly passing object expressions to the result of the macro, we
can first make the compiler type them and then find out if they have the proper
type. The body of the above \kw{if} macro should be

\begin{verbatim}
  def tcond = TypedExpr (cond);
  def te1 = TypedExpr (e1);
  def te2 = TypedExpr (e2);
  if (tcond.Type == <[ ttype: bool ]> ) {
    <[ 
      match ($(tcond : typed)) { 
        | true => $(te1 : typed) 
        | false => $(te2 : typed) 
      } 
    ]>
  }
  else
    FailWith ("`if' condition must have type bool, " +
              "while it has " + tcond.Type.ToString())
\end{verbatim}
%$

Note that typed expressions are used again in the quotation, but with a special 
splicing tag ``typed''. This means that the compiler does not have to perform the 
typing (in fact, it cannot do this from now on) on the provided syntax trees. 
Such a notation introduces some kind of laziness in typing, which is guided
directly by a programmer of the macro.

\section{How it works}
We will now describe how our meta-programming system works internally. 

Each macro is translated to a separate class implementing a special
\emph{IMacro} interface. It provides a method to run the macro, which
in most cases involves passing it the list of \nem\ grammar elements
(untyped syntax trees of object programs).

Therefore at the compiler level macro is a function operating on
syntax trees. There are several kinds of syntax trees used in Nemerle
compiler. We will be interested in parse trees and typed trees.

Parse trees are generated by the parser as well as the quotations during
macro execution. They are mostly one-to-one with the grammar of the
language.

Typed trees contain less language construct (particularly no macro
invocations). These constructs are however more explicit. In particular
they contain inferred types of expressions.

The process of typing in compiler generally involves rewriting parse
trees into typed trees.

The typing function, when it encounters macro invocation, executes the
\emph{Run} method of respective macro object. Macro invocation looks
like a regular function call, so we distinguish these two cases by
looking up name of invoked function in list of currently loaded macros
(the \emph{IMacro} interface has a special \emph{GetName} method).

To support typing parts of macro parameters, we a special node in parse
tree, that simply holds typed tree node. Typing function simply strips
parse tree box, leaving content intact.


\subsection{Quotations}
The quotation system is just a shortcut for explicitly constructing syntax
trees from compiled data types.  For example \verb,f(x),
expression is internally represented by 
\verb.E_call (E_ref ("f"), [Parm (E_ref ("x"))]).,
which is equivalent to \verb,<[ f (x) ]>,. Translating the quotation involves
``lifting'' syntax tree by one more level -- we are given an expression 
representing a program (its syntax tree) and we must create a representation 
of such expression (larger syntax tree).
% cale poprzednie zdanie niejasne niestety. nastepne tez. E.
% hmm, to wlasnie jest dosyc zakrecone, moze teraz jest lepiej
This implies building a syntax tree of the given syntax tree, like

\begin{verbatim}
  E_call (E_ref ("f"), [Parm (E_ref ("x"))] 
  =>
  E_call ("E_call", 
    [Parm (E_call ("E_ref", [Parm (E_literal 
       (L_string ("f")))]));
     Parm (E_call ("Cons", [Parm (E_call ("Parm", 
       [Parm (E_call ("E_ref", 
          [Parm (E_literal (L_string ("x")))]))]))]))])
\end{verbatim}

\noindent
or using the quotation

\begin{verbatim}
  <[ f (x) ]> 
  =>
  <[ E_call (E_ref ("f"), [Parm (E_ref ("x"))]) ]>
\end{verbatim}

Now splicing means just ``do not lift'', because we want to pass the value of 
the meta-language expression as the object code. Of course it is only valid 
when such an expression describes (is type of) the syntax tree. Operator \verb,.., 
inside the quotation is translated as the syntax tree of list containing lifted 
expressions from the provided list (which must occur after \verb,..,).


% ja bym tego nie dawal, nie wiadomo czy cokolwiek sie uda, szczegolnie
% ze np. 1+true wcale nie musi byc blednym kodem w jakies innej pogrzanej
% wersji mscorlib
% --Michal
\subsection{Typing object code}

To detect as many errors as possible during macro compilation (as opposite to
macro execution) we support special kind of typing function for macros.
Of course we cannot tell the type of the \verb,$(),-splices in the object
code (it is obviously undecidable), but still we can reject programs like
\verb,<[ 1 + true ]>,.

Typing function simply skips over types of splices, yet unbound
identifiers, and similar constructs, giving them $\forall \alpha
. \alpha$ type. It is fairly permissive, that is it will accept any
piece of program that can have a valid type.

This feature is still under heavy design, and some details are not decided yet.


\subsection{Making identifiers hygienic}
In this section we describe how we achieve hygiene in our system.
As said before we use distinct ``namespace'' or ``color'' for each macro
invocation. Our coloring system is quite simple.  All plain identifiers
introduced in quotation receive color of current macro invocation. The
identifiers marked with \verb,$(id : var), receive color of code that
called the macro. This can be color of top-level object code, as well
as color of some earlier quotation.

We describe our approach formally using a few simple inference rules.

The entire language is flattened to plain flat terms. We are only
interested here in identifiers and macro invocations.  Macro invocation
are written as terms of the form $macro(name, parameter)$, while
identifiers are denoted $id(v,c,g)$ where $v$ is the name of identifier,
$c$ is its color, and $g$ is global environment for given symbol.

Name is term representation of strings written in program text. Color
is term representation of integer, but it can take two special forms
$current()$ and $usesite()$. The global environment is list of namespaces
opened with the \verb,using, declaration in scope where given identifier
was defined.

After all macros are expanded and colors resolved we can say what each
name binds to. Regular lexical scoping rules apply -- some terms define
symbols, some other use it. Use refers to nearest preceding declaration
with the same color. If there is no such declaration -- the symbol is
looked up in global environment enclosed in each symbol.

$\Gamma$ is a function describing dynamic semantics of compiled macros.
It takes name of macro and its parameter as input and returns result of
macro application on this parameter. If macro needs more parameters they
can be easily encoded using cons-like terms.

Identifiers introduced by macros in a hygienic way are denoted
$id(v,current(),g)$ while identifiers introduced with \verb,UseSiteSymbol,
are marked $id(v,usesite(),g)$ in result of this function. The
global environment $g$ there comes from context within which macro
was defined. Top-level object code is already colored, with a single
unique color.


$$
  \infrule{
    \nf{e_1}{e_1'} \ \ldots\ \nf{e_n}{e_n}
  }{
    \nf{
      f(e_1, \ldots, e_n)
    }{
      f(e_1', \ldots, e_n')
    }
  }{(Mon) \ \ \textrm{where}\ f \notin \{ macro, id \}}
$$

$$
  \infrule{
  }{
    \nf{
      id (v, x, g)
    }{
      id (v, x, g)
    }
  }{(MonId)}
$$

$$
  \infrule{
    \colr{\Gamma(m,e)}{e'} \ \ \ \nf{e'}{e''}
  }{
    \nf{
      macro (id(m, u, g), e)
    }{
      e''
    }
  }{(Expand) \ \ \textrm{where}\ c \ \textrm{is\ a\ fresh\ color}}
$$

$$
  \infrule{
    \colr{e_1}{e_1'} \ \ldots\ \colr{e_n}{e_n}
  }{
    \colr{
      f(e_1, \ldots, e_n)
    }{
      f(e_1', \ldots, e_n')
    }
  }{(ColMon)\ \ \textrm{where}\ f \notin \{ id \}}
$$

$$
  \infrule{
  }{
    \colr{
      id (v, usesite (), g)
    }{
      id (v, u, g)
    }
  }{(ColUse)}
$$

$$
  \infrule{
  }{
    \colr{
      id (v, current (), g)
    }{
      id (v, c, g)
    }
  }{(ColCur)}
$$

$$
  \infrule{
  }{
    \colr{
      id (v, x, g)
    }{
      id (v, x, g)
    }
  }{(ColSkip)\ \ \textrm{where}\ x \notin \{ current (), usesite () \}}
$$

\textsc{Definition}: We say that $e$ is \emph{valid} if it does not
contain terms of the form $id(v, current(), g)$ and $id(v, usesite(), g)$.

\textsc{Definition}: We say that $e$ is in \emph{normal form} if it is
valid and does not contain $macro$ in head of any term.

Normal form is thus object code without macro invocation and with full
color information.

\textsc{Theorem}: for any valid $e$, there exists is $e'$ in normal form,
such that it can be proven that $e \rightarrow e'$.

\textsc{Proof}: rules for both $\rightarrow$ and $\Rightarrow$ are
syntax-directed and defined for all terms. Thus for any $e$ there exists
$e'$, such that $e \rightarrow e'$. Moreover usage of $\rightarrow$
eliminates all occurrences of $macro()$, usage of $\Rightarrow$ guarantees
elimination of all $current()$ and $usesite()$ introduced by macros.

\subsection{Compiling and loading}
A key element of our system is the execution of meta-programs during 
the compile-time. To do this they must have an executable form and be
compiled before they are used.

Macros after compilation are stored in assemblies (compiled libraries
of code). All macros defined within an assembly are listed in its metadata.
Therefore when user requests linking an assembly during the compilation,
we can construct instances of all macro classes and register them by
names within the compiler.

Each macro resides in a namespace. The name of the macro is prefixed
with the name of the namespace. To use short name of macro one need
to issue \verb,using, declaration for the respective namespace. This
works exactly the same as for regular functions. If macro defines a
syntax extension, it is activated only if \verb,using, for respective
namespace is in force.

This is an industrial strength design. It allows macro usage without namespace
pollution, syntax extensions can be loaded on demand, and all macros
can be used by programmers unaware of the very idea of meta-programming.

\subsection{Separate Compilation}
Current implementation requires macros to be compiled in separate pass,
before compilation of program that uses them. This results in inability
to define and use given macro in the same compilation unit. While we
are still researching the field of generating and running macros during
the same compilation our current approach also has some advantages.

The most important one is that it is simple and easy to understand --
one need to first compile the macros (probably being integrated into
some library), and then load them into the compiler and finally use
them. This way stages of compilation are clearly separated in a well
understood fashion -- an important advantage in industrial environment
where meta-programming is a new and still somewhat obscure topic.

The main problem with ad-hoc macros (introduced and used during the
same compilation) is that we need to first compile transitive closure
of types (classes with methods) used by given macro. This very macro
of course cannot be used in these types.

This issue may be hard to understand by programmers (``why my program
doesn't compile after I added new field to this class?!''). On the
other hand such a dependency-closed set of types and macros can be
easily split out of the main program into the library.



\section{Related work}
\subsection{Scheme hygienic macros}
Our system has much in common with modern Scheme macro expanders \cite{Scheme:HygienicAlg}:
\begin{itemize}
\item Alpha-renaming and binding of variables is done after macro expansion, using
      context stored in macro in use site
\item Macros can introduce new binding constructs in a controlled way,
      without possibility to capture third party names.
\item Call site of macros has no syntactic baggage, the only place where
      special syntax appears is macro definition -- this implies simple
      usage of macros by programmers not aware of meta-programming.
\end{itemize}

Still maintaining above features we embedded macro system into a statically typed
language. Generated code is type-checked after expansion. We also provide clear
separation of stages -- meta-function must be compiled and stored in some library
before use.

Works on Scheme last quite long, and many interesting features has been proposed.
For example first-class macros in \cite{Macros:FirstClass} seem to be possible
to implement in Nemerle by simply passing funtions operating on object code.

\subsection{Tamplate Haskell}
There are interesting differences between Template Haskell \cite{Haskell:Meta} and Nemerle macros:
\begin{itemize}
\item Resolving bindings during translating of quotations brings ability to reason
  about type-correctness of object code, before it is used. It allows detecting
  errors much earlier. Nevertheless presence of \verb,$, splicing construct makes %$
  typing be postponed to next stage of compilation, in which case new bindings 
  must be dynamic.
\item Template Haskell macros are completely higher-order, like any other 
  function: they can be passed as arguments, partially applied, etc. This
  however requires manually annotating which code should be executed at 
  compile-time. We decided to make macros callable only by name (like in Scheme), 
  so their usage looks like calling an ordinary function. We are still able to use 
  higher-order functions in meta-language (functions operating on object code can 
  be arbitrary), so only the top meta-function (prefixed with \verb,macro, is 
  triggering compile-time computations.
\item We do not restrict declaration splicing to top-level code and we also do not
  introduce special syntax for macros introducing them. This seems a good way of
  taking advantage of binding names after macro expansion and imperative style
  of Nemerle. It is natural for imperative programmer to think about introduced 
  definitions as side effects of calling macros, even if these calls resides within
  quoted code.
\item We introduce macros operating on type declarations, which are able to imperatively
  modify them. Moreover, they look like attributes attached to type definitions, so
  again programmer does not have to know anything about meta-programming to use them.
\end{itemize}

There are still many similarities to Template Haskell. We derive idea of quasi-quotation
and splicing directly from it. Also idea of executing functions during compilation
and later type-checking their results is inspired by Template Haskell.

\subsection{C++ templates}
% FIXME: \cite{}

\subsection{MacroML}
MacroML \cite{MacroML}, the proposal of compile-time macros on top of an ML language, 
has similar assumptions to Template Haskell by means of binding names in quotations before 
any expansion. It additionally enables pattern for introducing hygienic definition capturing
macro use-site symbols (similar to our \verb,UseSiteSymbol(),). All this is done
without need to break typing of quotation befor expansion.

Macros in MacroML are limited to constructing new code from given parts, so matching
and decomposing of code is not possible.

\subsection{MetaML}
MetaML \cite{MetaML} inspired both Template Haskell and MacroML by introducing 
quasi-quotation and idea of typing object code. It was developed mainly to operate 
on code and execute it during runtime, so it represents a little different field of 
research than ours.

\section{Further work}
\begin{itemize}
\item Runtime program generation -- optimizations for runtime accessible data only,
   dynamic profiling of long running code, etc.
\item FIXME: AOP
\end{itemize}

\paragraph{Acknowledgments}
We would like to thank Marcin Kowalczyk for very constructive discussion about hygienic
systems, Lukasz Kaiser for useful opinions about quotation system and Ewa Dacko for
corrections of this paper.

%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
\bibitem {Scheme:HygienicAlg}
Dybvig, R. K., Hieb, R., Bruggeman, C.:
Syntactic Abstraction in Scheme.
Lisp and Symbolic Computations, 1993

\bibitem {Haskell:Meta}
Sheard, T., Jones, S. P.:
Template Meta-programming for Haskell.
Haskell Workshop, Oct. 2002, Pittsburgh

\bibitem {Lisp:Macros}
Steele, Jr., Guy, L.
Common Lisp, the Language.
Digital Press, second edition (1990)

\bibitem {Scheme:Macros}
Clinger, William, Rees, Jonathan et al.
The revised report on the algorithmic language Scheme.
LISP Pointers, 4, 3 (1991)

\bibitem {Scheme:Compilable}
Flatt, M.:
Composable and Compilable Macros.
ICFP, Oct. 2002, Pittsburgh

\bibitem {MacroML}
Ganz, S., Sabry, A., Taha, W.:
Macros as multi-stage computations: type-safe, generative, binding macros in MacroML
Preceedings of the ACM SIGPLAN International Conference on Functional Programming (ICFP-2001),
New York, Sep. 2001

\bibitem {MetaML}
Sheard, T., Benaissa, Z., Martel, M.:
Introduction to multistage programming: Using MetaML

\bibitem {Meta:Accomplishments}
Sheard, T.:
Accomplishments and Research Challenges in Meta-Programming
2001

\bibitem {Macros:FirstClass}
Bawden, A.:
First-class Macros Have Types
Symposium on Principles of Programming Languages, 2000

\bibitem {AspectJ}
AspectJ

\end{thebibliography}

\end{document}
