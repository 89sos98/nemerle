% Meta-programming in Nemerle
%
\documentclass{llncs}
%
\newcommand{\net}[0]{{\tt .NET}}
\newcommand{\netf}[0]{{\tt .NET} framework}
\newcommand{\nem}[0]{Nemerle}
\newcommand{\cs}[0]{C\#}
\newcommand{\oo}[0]{object-oriented}
\newcommand{\kw}[1]{{\tt \bf #1}}
%
\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
%
\title{Meta-programming in \nem}
%
\titlerunning{Meta-programming in Nemerle}  
%
\author{Kamil Skalski \and Michal Moskal \and Pawel Olszta}
%
\authorrunning{Kamil Skalski et al.} 
%
%
\institute{University of Wroclaw, Poland \\
           \texttt{http://www.nemerle.org/}}
%
\maketitle              % typeset the title of the contribution
%
\begin{abstract}
We present the design of a meta-programming system embedded into \nem\,,
a new functional language for the \net\ platform. The system enables
compile-time operations -- generation, transformation and automated 
analysis of programs by means of hygienic code quotation, syntax 
extensions, operating on the code like on any other datatype (e.g. listing, 
adding or changing members of class definition), performing partial 
typing of a program syntax tree (compiler internal typing procedures 
are executed by a macro code) and interoperability with the compilation 
process. All these operations can be fully parametrized with any 
external data (like a database, a file or a web page).

Our system is a convenient tool for Aspects Oriented Programming with 
the ability to operate on datatypes, traverse the program code and perform 
various algorithmic operations on its content.
\end{abstract}

\section{Introduction}
The idea of compile-time meta-programming has been studied for quite a long time.
It was incorporated into several languages, like Lisp macros \cite{Lisp:Macros}, 
Scheme hygienic macros \cite{Scheme:Macros}, C preprocessor-based macros, 
C++ template system and finally 
Haskell Template Meta-programming \cite{Haskell:Meta}. 
They vary in their capabilities and ease of use, but generally imply computations 
during compilation of the program and generating code from some definitions.

During this process programs are treated as \emph{object programs}, which are 
data supplied to \emph{meta-programs}. They can be then arbitrarily transformed or 
analyzed and the final result is compiled just like a regular program. These
operations may be repeated or take place in stages. In the latter case the 
generated programs can generate other programs and so on.

\emph{Meta-language} is a language for programming such operations. It 
usually has its own syntax for describing various constructs of the object language.
For example, in our system, \verb,<[ 1 + f (2 * x) ]>, denotes the syntax 
tree of expression \verb,1 + f (2 * x),. This idea is called \emph{quasi-quotation}. 
The prefix \emph{quasi} comes from the possibility of inserting values 
of meta-language expressions into the quoted context -- if \verb,g(y), is such 
an expression, we can write \verb.<[ 1 + $(g (y)) ]>., %$ 
which describes a syntax tree, whose second part is replaced by the result of 
evaluation of \verb,g(y),.

\subsection{Our contribution}
In this paper we introduce a few new ideas. They are mainly the continuation of the 
previous research on meta-programming in other languages, composed together to give 
a consistent and easy to use tool:
% ten kawalek bym chyba zmienila, zeby brzmial bardziej marketingowo...? = E.
% oj, tu ju≈º i tak wystapienie slowa easy, uniform, simple jest za duze IMHO

\begin{itemize}
  \item We develop a uniform, hygiene preserving and simple quasi-quotation system, 
    which does not require learning of internal compiler data structures to generate 
    and transform quite complicated object programs. It also provides an easy way to 
    write variable argument constructs (like tuples or function calls with an arbitrary 
    amount of parameters).
  \item Using macros is transparent from the user point of view -- the meta-program and 
    common function calls are indistinguishable, so the user can use the most complex
    macros prepared by others without even knowing the idea of meta-programming.
  \item Flexible definition of syntax extensions allows even more straightforward 
    embedding of macros into the language without interfering with compiler internals.
  \item Our system can be used to transform or generate practically any fragment
    of a program, which, composed with \net\ \oo\ structure, provides a powerful tool for
    software engineering methodologies like aspects-oriented programming.
  \item We allow macros to type fragments of the code, which they operate on, during
    their execution. This allows to parameterize them not only with the syntax of provided
    expressions, but also with the entire context of the program and types of those 
    expressions.
  \item The separate compilation of macro definitions provides the clean and manageable 
    stage separation.
\end{itemize}

\subsection{Characteristics of \nem\ meta-system}
Our meta-system has both \emph{program generation} and \emph{analysis}
capabilities \cite{Meta:Accomplishments}. It can easily walk through the abstract 
syntax tree of the object program and gather information about it as well 
as change it (often using gathered data).

The system is designed mainly for operating on object programs at the compile-time.
However, using features of \net\ and its dynamic code loading abilities, it is
also possible to execute macros during run-time. 

The meta-language is \emph{homogeneous}, which means that it is the same as the 
object language. We can use common \nem\ functions within macros and the syntax 
of generated programs is no different than the one used to write macros.

The quasi-quotation provides a clear separation of the object program from 
the meta-language. This manual annotation provides a way to separate the stages
of execution in a well understood fashion. It is also semantically clear, which 
part of the code is generated and which is generating. The symbols from the 
object-code are alpha-renamed so that they do not interfere with the external code. 

\section{First examples}
Suppose we want to add some new syntax to our language, like the \kw{for} loop.
We could embed it into the compiler, but it is a rather hard and inelegant way -- such 
addition is quite short and it should not involve much effort to complete. 
Here a macro can be used.

\begin{verbatim}
macro for (init, cond, change, body) 
{
  <[ 
    $init;
    def loop () {
      if ($cond) { $body; $change; loop() } 
      else () 
    }; 
    loop ()
  ]>
}
\end{verbatim}

This code creates a special meta-function, which is executed at the compile-time
in every place where its original call is placed. Its result is then inserted 
into the program. Always when something like

\begin{verbatim}
  for (i <- 0, i < n, i <- i + 2, a[i] <- i)
\end{verbatim}

\noindent
is written, the appropriate code is created according to the \kw{for} macro and 
replaces original call.

The macros may instruct the compiler to extend the language syntax -- for example 
a macro for the \kw{for} loop with a C-like syntax can be defined. Writing

\begin{verbatim}
  macro for (init, cond, change, body) 
  syntax ("for", "(", init, cond, change, ")", body) { ... }
\end{verbatim}

\noindent
would add a new rule to the parser, which allows using

\begin{verbatim}
  for (i <- 0; i < n; i <- i + 2) a[i] <- i
\end{verbatim}

\noindent
instead of the call mentioned above.

\subsection{Compiling sublanguages from strings}
Macros are very useful for the initial checking and processing of the code
written as strings in a program. This relates to many simple languages,
like \kw{printf} formatting string, regular expressions or even SQL, which are 
often used directly inside the program. 

Let us consider a common situation, when we want to parameterize an SQL query
with some values from our program. Most database providers in 
\netf\ allow us to write commands with parameters, but neither their syntax 
is checked during the compilation, nor the consistency of the SQL data types 
with the program is controlled.

With a well written macro, we could write
\begin{verbatim}
  sql_loop (conn, "SELECT salary, LOWER (name) AS lname"
                  "  FROM employees"
                  "  WHERE salary > $min_salary") {
    print ("$lname : $salary\n")
  }
\end{verbatim} %$

to obtain the syntax and type-checked SQL query and the following code
\begin{verbatim}
  def cmd = SqlCommand ("SELECT salary, LOWER (name)"
                        "  FROM employees"
                        "  WHERE salary > @parm1", conn);

  (cmd.Parameters.Add (SqlParameter ("@parm1", DbType.Int32)))
    .Value <- min_salary;
  def r = cmd.ExecuteReader ();
  while (r.Read ()) {
    def salary = r.GetInt32 (0);
    def lname = r.GetString (1);
    print ("$lname : $salary\n")
  }
\end{verbatim}

In fact the \verb,print, function here is another macro, that generates 
the following code, basing on the given string parameter:

\begin{verbatim}
  System.Console.Write (lname.ToString ());
  System.Console.Write (" : ");
  System.Console.Write (salary.ToString ());
  System.Console.Write ("\n")
\end{verbatim}

\section{Variable amount of arguments}
The quotation provides full freedom in constructing any kind of expression.
For example, we can decompose a tuple of any size and print its elements.

\begin{verbatim}
  macro PrintTuple (tup, size : int)
  {
    def symbols = array (size);
    mutable pvars <- [];
    for (mutable i <- size - 1; i >= 0; i <- i - 1) {
      symbols[i] <- NewSymbol ();
      pvars <- <[ pattern: $(symbols[i] : var) ]> :: pvars;
    };
    mutable exps <- [];
    for (mutable i <- size - 1; i >= 0; i <- i - 1)
      exps <- <[ WriteLine ($(symbols[i] : var)) ]> :: exps;

    exps <- <[ def (.. $pvars) = $tup ]> :: exps;
    <[ {.. $exps } ]>
  }
\end{verbatim} %$

Note that here we need a number describing the size of the tuple. We show later, 
how to obtain the type of the given expression within the macro. This, for 
example, allows to compute the size of the tuple described by the \verb,tup, variable.

\section{Pattern matching on programs}
The quotation can be used to analyze the program structure as easily as generate
it. Standard mechanisms of the language, like the pattern matching, fit perfect
for such a purpose. 

Suppose we want to check the code for occurrences of the following buggy
fragments:
\begin{verbatim}
 if ( foo != null && foo.bar || foo.blah ) foo.blah
\end{verbatim}

We write a macro, which inspects the given program to signal such constructs
(this is a simplified version, that will warn more then actually needed).
\begin{verbatim}
macro analyze (exp) : void 
{
  def traverse (ex, checked : Set<Name>) : Set<Name> {
    match (ex) {
      | <[ $(name : var) != null ]> => 
        checked.Add (name)
      | <[ $(name : var).$(_) ]> => 
        if (!checked.Contains (name)) 
          throw MaybeBug() 
        else 
          checked
      | <[ $e1 && $e2 ]> => 
        traverse (e2, traverse (e1, checked))
      | <[ $e1 || $e2 ]> => 
        def chck1 = traverse (e1, checked);
        def chck2 = traverse (e2, checked);
        Intersect (chck1, chck2)
      | <[ if ($cond) $e1 else $e2 ]> =>
        traverse (e1, traverse (cond, checked));
        traverse (e2, checked)
      | _ => Macros.ApplyExpr (exp, fun (x) { 
          traverse (x, checked) 
        })
    }
  };
  Macros.ApplyExpr (exp, fun (x) { traverse (x, Set ()) })
}
\end{verbatim} %$

\section{Macros on declarations} \label{Declarations}
Macros can operate not only on expressions, patterns, types, but also on
any part of language, like classes, interfaces, other type declarations,
methods, etc. The syntax for those operations is quite different. Again, we
treat language constructs as data objects, which can be transformed, but
this is not done entirely with quotations. We use a special API, designed 
basing on \verb,System.Reflection,, which is used in \net\ for dynamic
generation of assemblies. \nem\ type system and operations we are performing
on them are not fully compatible with the interface of \verb,Reflection, 
(we cannot directly derive from its classes), but are quite similar.

With such a tool we can analyze, generate or change any declaration
in the program. For example, dump definition of each data structure 
to an XML file, create serialization methods or automatically
generate fields or methods from an external description.

Such macros are not called like ordinary functions, but are added as
attributes in front of the declaration, similarly as \cs\ attributes.

\begin{verbatim}
  [SerializeBinary ()] 
  public module Company {
    [ToXML ("Company.xml")] 
    public class Employee {
      ...
    }

    [FromXML ("Product.xml"), Comparable ()] 
    public class Product { }
  }
\end{verbatim}

\subsection{Transforming types}
Macros that operate on declarations change them in the imperative fashion.
Their first parameter is always an object representing the given 
declaration. 

\begin{verbatim}
  macro ToXML (c : Type, file : string) { 
\end{verbatim}

We can easily list the data contained in the provided object, like fields 
or methods of a class and add a new method using their names.

\begin{verbatim}
  def fields = c.GetFields ();
  def methods = c.GetMethods ();
  def method_builder = 
    c.DefineMethod ("ToXML", MethodAttributes.Public, 
                    <[ type: void ]>, []);
  def list_fields = 
    List.Map (fields, fun (x) { <[ 
      xml_writer.WriteAttributeString ($(x.Name.GetId () : string), 
                                       $(x.Name : var).ToString ()) 
    ]> }
  );
  method_builder.SetBody (<[
    def xml_writer = XmlTextWriter ($(file : string), null);
    { ..$list_fields };
    xml_writer.Close ()
  ]>)
\end{verbatim}

With the macro above (perhaps modified to do some additional formatting)
we can generate serialization methods for any class simply by adding 
the \verb,[ToXML ("file.xml")], attribute.

\section{Aspects-oriented programming}
AOP has been proposed as a technique for separating of different 
\emph{concerns} in software. It is often a problem, when they crosscut 
natural modularity of the rest of implementation and must be written 
together in code, leading to tangled code. 

In contrary to many other systems, which define fixed \emph{join points}
in the code (like AspectJ \cite{AspectJ}), macros allow to place the user code in 
arbitrary parts of program. One can write a macro traversing all the classes
and adding some behavior to their bodies. Defining of these points is not
restricted to any design, but the user can write a program that defines them.

In general it should be possible to implement most Aspects-oriented
and Adaptive-programming system design with more or less complex 
macros. This field is our future research direction and we will present
more details after finishing the implementation of all the necessary features.

\section{Details of our design}
In this section a more formal definition of the macro and our meta-system is provided.

A macro is a top-level function prefixed with the \kw{macro} keyword, 
which may have access modifiers like other methods (\verb,public,, \verb,private, etc.) 
and resides in \net/\nem\ namespace. It is used within the code like
any other method, but it is treated in a special way by the compiler.

Its formal parameter types are restricted to the set of \nem\
grammar elements (including simple literal types). Parameters of 
the macro are always passed as their syntax trees, which for some
types are partially decomposed. For example, literal types appear
within the macro as real values (\verb,int,, \verb,string,, etc.),
but they are passed
as syntax trees, so they must be given as constant literals (it is
obvious since these values must be known at the compile-time).

\subsection{Macro as a function}
A macro cannot be called recursively or passed as a first-class citizen
(although it can generate the code, which contains the calls of this macro).
Still, it can use any function from the rest of the program in a standard ML
fashion, so we consider this as a minor disadvantage. If complex 
computations on syntax trees are necessary, one must simply put them 
into some arbitrary functions and run the entire computation from within 
the macro. Such design allows to define easily which functions are run 
at the compile-time without requiring any special annotations at their use site.

\subsection{Names binding in quotation}
A very important property of the meta-system is called \emph{hygiene} 
and relates to the problem with names capture in Lisp macros, resolved 
later in Scheme. It specifies that variables introduced by a macro 
may not bind to variables used in the code passed to this macro. Particularly 
variables with the same names, but coming from different contexts, should be 
automatically distinguished and renamed.

Consider the following example:

\begin{verbatim}
  macro identity (e) { <[ def f (x) { x }; f($e) ]> }
\end{verbatim} %$

Calling it with \verb,identity (f(1)), might generate a confusing code like

\begin{verbatim}
  def f (x) { x }; f (f (1))
\end{verbatim}

To prevent names capture, all macro-generated variables should be renamed 
to their unique counterparts, like in

\begin{verbatim}
  def f_42 (x_43) { x_43 }; f_42 (f (1))
\end{verbatim}

In general, names in the generated code bind to definitions visible within 
their scope. The binding is done after all transformations during the execution
of the macro are finished. This means that a variable used in a quotation 
may not necessarily refer to the definition visible directly in the place 
where it is written. Everything depends on where it occurs in the finally 
generated code. Consider the following example:

\begin{verbatim}
  macro foo (body) {
    def d1 = <[ def mail = login + "@nemerle.org" ]>;
    def d2 = <[ def login = $(developer : string) ]>
    <[ $d2;
       $d1;
       def msg = MailMessage ();
       msg.To <- mail;
       msg.Body <- $body;
       SmtpMail.Send (msg)
    ]>
  }
\end{verbatim}

As macros might get large and complex it is frequently very useful to compute
parts of the expression independently and then compose the final code from them.
Still, names in such macro are alpha-renamed so that they do not capture any 
external definitions. The renaming is defined as putting names created 
in the single macro execution into the same ``namespace'', which is
mutually exclusive with all other ``namespaces'' and the top-level code.
This is exactly the hygiene rule -- neither the macro can capture names
used in the macro-use place nor it can define anything colliding with the 
external code. 

This is an opposite approach to Template Haskell \cite{Haskell:Meta}, 
where the lexical scoping means binding variables from the object code immediately 
to definitions visible at the construction site of the quotation.
We find our approach more flexible, as we can transform the code with much
more freedom, while still keeping the system hygienic. This is of course just 
a design decision, naturally associated with certain costs. Sometimes it is not 
obvious to recreate bindings simply by looking on the code, but here we assume 
that a programmer of a macro knows the structure of the code to be generated. 
We also lose the ability to detect some errors earlier, but as they are always 
detected during the compilation of the generated code, we believe it is 
a minor disadvantage. 

One can think, that putting all identifiers from entire macro invocation into
a single ``namespace'' is not a good idea. Especially when we use some general 
purpose code generating function from library, which should generate only its 
own unique names. To obtain such independent, hygienic functions we write

\begin{verbatim}
[Hygienic] f (x : Expr) : Expr { ... }
\end{verbatim}

The \verb,[Hygienic], attribute is a simple macro, which transforms \verb,f, 
to enable its own context during execution. This way function receives the
same semantics as macro with regards to hygiene. We consider this behavior
not good by default, because code is often generated by some tool functions,
especially defined locally in macro and they should not change their context.

\subsection{Breaking hygiene}
Sometimes it is useful to share some names between few macro executions.
It can be done safely by generating a unique identifier independent of macro
executions. We support it by function \verb,NewSymbol (), whose return value
can be stored in a variable, providing the hygiene preserving solution.

There are also situations, where we know the exact name of the variable used in 
the code passed to the macro. If we wanted to define a name referring to it, we 
would have to change the scope to our macro use site. As it breaks hygiene, 
it should be done in a controlled manner. Consider a macro introducing \kw{using} 
keyword (C\# keyword, simplified for the purpose of this paper):

\begin{verbatim}
macro using (name : string, val, body) {
  def v = Macros.UseSiteSymbol (name);
  <[ 
    def $(v : var) = $val;
    try $body finally $v.Dispose ()
  ]>
}
\end{verbatim}

It should define a symbol binding to variables of the same name in \verb,body,.
But if it contained some other external code, like in:

\begin{verbatim}
macro bar(ext) { 
  <[ using ("x", Foo (), { $ext; x.Compute () }) ]> 
}
\end{verbatim} %$

\noindent
some inadvertent capture of variables in \verb,ext, might happen if 
\verb,x, was just a plain dynamically scoped variable.

Although it is not recommended, also nonhigienic symbols can be created, by
\verb,$(x : dyn),, where \verb,x, is of type \verb,string,. %$
They are bound to the nearest definition with the same name appearing in the
generated code, regardless of the context it comes from.

\subsection{Global symbols}
The object code often refers to variables, types or constructors imported
from other modules (e.g. \net\ standard library or symbols defined in 
the namespace where the macro resides). In normal code we can omit the prefix 
of the full name, by including \kw{using} keyword, which imports symbols from
given namespace. Unfortunately this feature used in the object code like

\begin{verbatim}
  using System.Text.RegularExpressions;

  public module Finder 
  {
    public static current : string;

    macro finddigit (x : string)
    {
      <[ 
        def numreg = Regex (@"\d+-\d+");
        def m = numreg.Match (current + x);
        m.Success ();
      ]>
    }
  }
\end{verbatim} %$

\noindent
brings some dependency on currently imported namespaces. We would like the
generated code to behave alike no matter where it used, thus the
\verb,Regex, constructor and the \verb,current, variable should be expanded to
their full name -- \verb,System.Text.RegularExpressions.Regex,
and \verb,Finder.current,, respectively. This operation is automatically
done by the quotation system. When a symbol is not defined locally (and with 
the same context as described in the previous section), its binding is 
looked for in global symbols imported at the quotation definition site.

Note that there is no possibility to override security permissions this
way. Access rights from the lexical scope of the macro are not exported to 
the place where the generated code is used. \net\ policies do not allow this, 
thus the programmer must not generate a code breaking the static security.

\subsection{Accessing compiler internals}
It is vital for meta-functions to be able to use all benefits they have
from running at the compile-time. They can retrieve information from the
compiler, use its methods to access, analyze and change data stored
in its internal structures. 

\subsubsection{Retrieving declarations}
For example, we can ask the compiler to return the \emph{type declaration} 
of a given \emph{type}. It will be available as the syntax tree, just like 
as we put a special macro attribute (Section \ref{Declarations}) before that declaration.
Certainly, such a declaration must not be an external type 
and has to be available within the compiled program as a source code.

\begin{verbatim}
  def decl = Macros.GetType (<[ type: Person ]>);
  xmlize (decl);    // we can use macros for declarations
\end{verbatim}

\section{Typing during execution of macro}
Some more advanced techniques are also possible. They involve a closer
interaction with the compiler, using its methods and data structures
or even interweaving with internal compilation stages.

For example, we can ask the compiler to type some of object programs,
which are passed to the macro, retrieve and compare their types, etc.
This means that we can plug between many important actions performed by 
the compiler, adding our own code there. It might be just a nicer 
bug reporting (especially for macros defining complex syntax extensions), 
making code generation dependent on input program types or improving
code analysis with additional information.

\subsection{Example}
Let us consider the following translation of the \kw{if} condition to the standard
ML matching:

\begin{verbatim}
  macro @if (cond, e1, e2)
  syntax ("if", "(", cond, ")", e1, "else", e1) 
  {
    <[ 
      match ($cond) {
        | true => $e1
        | false => $e2
      }       
    ]>
  }
\end{verbatim} % $

When \verb,if ("bar") true else false, was written, the compiler would comply
that type of matched expression is not of type \verb,bool,. Such an error 
message could be very confusing, because the programmer may not know, that his 
\verb,if, statement is being transformed to the \verb,match, statement. Thus, 
we would like to check such errors during the execution of the macro, so we can
generate more verbose message.

\subsection{Usage}
Instead of directly passing object expressions to the result of the macro, we
can first make the compiler type them and then find out if they have the proper
type. The body of the above \kw{if} macro should be

\begin{verbatim}
  def tcond = TypedExpr (cond);
  def te1 = TypedExpr (e1);
  def te2 = TypedExpr (e2);
  if (tcond.Type == <[ ttype: bool ]> ) {
    <[ 
      match ($(tcond : typed)) { 
        | true => $(te1 : typed) 
        | false => $(te2 : typed) 
      } 
    ]>
  }
  else
    FailWith ("`if' condition must have type bool, " +
              "while it has " + tcond.Type.ToString())
\end{verbatim}
%$

Note that typed expressions are used again in the quotation, but with a special 
splicing tag ``typed''. This means that the compiler does not have to perform the 
typing (in fact, it cannot do this from now on) on the provided syntax trees. 
Such a notation introduces some kind of laziness in typing, which is guided
directly by a programmer of the macro.

\section{How it works}
We will now describe how our meta-programming system works internally. 

Each macro is translated to a separate class implementing a special interface. 
It provides a method to run the macro, which in most cases involves passing
it the list of \nem grammar elements (untyped syntax trees of object 
programs). 

\subsection{Operating on syntax trees}
When we consider object programs passed to the \emph{Run} function as datatypes
of compiler internal representation of syntax trees, then macros are 
simply functions operating on them. In the matter of fact, the programmer can
open compiler data namespaces directly and build syntax trees using its
constructors. 

The quotation system is just a shortcut for doing this. For example \verb,f(x),
expression is internally represented by 
\verb.E_call (E_ref ("f"), [Parm (E_ref ("x"))]).,
which is equivalent to \verb,<[ f (x) ]>,. Translating the quotation involves
``lifting'' syntax tree by one more level -- we are given an expression 
representing a program (its syntax tree) and we must create a representation 
of such expression (larger syntax tree).
% cale poprzednie zdanie niejasne niestety. nastepne tez. E.
% hmm, to wlasnie jest dosyc zakrecone, moze teraz jest lepiej
This implies building a syntax tree of the given syntax tree, like

\begin{verbatim}
  E_call (E_ref ("f"), [Parm (E_ref ("x"))] 
  =>
  E_call ("E_call", 
    [Parm (E_call ("E_ref", [Parm (E_literal 
       (L_string ("f")))]));
     Parm (E_call ("Cons", [Parm (E_call ("Parm", 
       [Parm (E_call ("E_ref", 
          [Parm (E_literal (L_string ("x")))]))]))]))])
\end{verbatim}

or using the quotation

\begin{verbatim}
  <[ f (x) ]> 
  =>
  <[ E_call (E_ref ("f"), [Parm (E_ref ("x"))]) ]>
\end{verbatim}

Now splicing means just ``do not lift'', because we want to pass the value of 
the meta-language expression as the object code. Of course it is only valid 
when such an expression describes (is type of) the syntax tree. Operator \verb,.., 
inside the quotation is translated as the syntax tree of list containing lifted 
expressions from the provided list (which must occur after \verb,..,).

One more thing to consider are the typed trees possibly stored in a result
of the executed macro. We need a special element in the untyped syntax tree, 
which holds the typed tree inside and which is treated in a special way during
the real typing by compiler. It is dereferenced and took as if it would be the 
result of recursive typing procedure. 
% niejasne, bylo: It is dereferenced and took as it would be the result of 
% recursive typing procedure. - E.
% hmm, chodzi o to, ze otypowany kawalek jest wyciagany i wstawiany tak, jak gdyby
% byl rezultatem normalnej procedury typujacej (czyli tak, jakby ona go zwrocila)

\subsection{Typing object code}
% bedziemy mieli cos takiego? np. odrzucenie <[ 1 + true ]>, z bindowaniem zmiennych
% po ekspansji i $ co prawda potrafimy niewiele powiedzie~, ale mo~e jednak? 
% np. ($(x : var) : Foo) ju~ ma jakie~ constrainty...

\subsection{Typing generated code}
FIXME: OUR HYPERSPACE COLORING, CONTEXT MARKING ALGORITHM

\subsection{Compiling and loading}
A key element of our system is the execution of meta-programs during 
the compile-time. To do this they must have an executable form and be compiled 
before they are used. 

FIXME: DESCRIBE OUR RESTRICTED APPROACH
FIXME: WRITE IT DOWN! (MICHAL!)

\section{Related work}
\subsection{Scheme hygienic macros}
Our system has much in common with modern Scheme macro expanders \cite{Scheme:HygienicAlg}:
\begin{itemize}
\item Alpha-renaming and binding of variables is done after macro expansion, using
      context stored in macro in use site
\item Macros can introduce new binding constructs in a controlled way,
      without possibility to capture third party names.
\item Call site of macros has no syntactic baggage, the only place where
      special syntax appears is macro definition -- this implies simple
      usage of macros by programmers not aware of meta-programming.
\end{itemize}

Still maintaining above features we embedded macro system into a statically typed
language. Generated code is type-checked after expansion. We also provide clear
separation of stages -- meta-function must be compiled and stored in some library
before use.

Works on Scheme last quite long, and many interesting features has been proposed.
For example first-class macros in \cite{Macros:FirstClass} seem to be possible
to implement in Nemerle by simply passing funtions operating on object code.

\subsection{Tamplate Haskell}
There are interesting differences between Template Haskell \cite{Haskell:Meta} and Nemerle macros:
\begin{itemize}
\item Resolving bindings during translating of quotations brings ability to reason
  about type-correctness of object code, before it is used. It allows detecting
  errors much earlier. Nevertheless presence of \verb,$, splicing construct makes %$
  typing be postponed to next stage of compilation, in which case new bindings 
  must be dynamic.
\item Template Haskell macros are completely higher-order, like any other 
  function: they can be passed as arguments, partially applied, etc. This
  however requires manually annotating which code should be executed at 
  compile-time. We decided to make macros callable only by name (like in Scheme), 
  so their usage looks like calling an ordinary function. We are still able to use 
  higher-order functions in meta-language (functions operating on object code can 
  be arbitrary), so only the top meta-function (prefixed with \verb,macro, is 
  triggering compile-time computations.
\item We do not restrict declaration splicing to top-level code and we also do not
  introduce special syntax for macros introducing them. This seems a good way of
  taking advantage of binding names after macro expansion and imperative style
  of Nemerle. It is natural for imperative programmer to think about introduced 
  definitions as side effects of calling macros, even if these calls resides within
  quoted code.
\item We introduce macros operating on type declarations, which are able to imperatively
  modify them. Moreover, they look like attributes attached to type definitions, so
  again programmer does not have to know anything about meta-programming to use them.
\end{itemize}

There are still many similarities to Template Haskell. We derive idea of quasi-quotation
and splicing directly from it. Also idea of executing functions during compilation
and later type-checking their results is inspired by Template Haskell.

\subsection{C++ templates}
% FIXME: \cite{}

\subsection{MacroML}
% FIXME: \cite{}
MacroML, the proposal of compile-time macros on top of an ML language, has similar 
assumptions to Template Haskell by means of binding names in quotations before any 
expansion. It additionally enables pattern for introducing hygienic definition capturing
macro use-site symbols (similar to our \verb,UseSiteSymbol(),). All this is done
without need to break typing of quotation befor expansion.

Macros in MacroML are limited to constructing new code from given parts, so matching
and decomposing of code is not possible.

\subsection{MetaML}
% FIXME: \cite{}
MetaML inspired both Template Haskell and MacroML by introducing quasi-quotation and
idea of typing object code. It was developed mainly to operate on code and execute it
during runtime, so it represents a little different field of research.

\section{Further work}
\begin{itemize}
\item Runtime program generation -- optimizations for runtime accessible data only,
   dynamic profiling of long running code, etc.
\item FIXME: AOP
\end{itemize}

\paragraph{Acknowledgments}
We would like to thank Marcin Kowalczyk for very constructive discussion about hygienic
systems, Lukasz Kaiser for useful opinions about quotation system and Ewa Dacko for
corrections of this paper.

%
% ---- Bibliography ----
%
\begin{thebibliography}{}
%
\bibitem {Scheme:HygienicAlg}
Dybvig, R. K., Hieb, R., Bruggeman, C.:
Syntactic Abstraction in Scheme.
Lisp and Symbolic Computations, 1993

\bibitem {Haskell:Meta}
Sheard, T., Jones, S. P.:
Template Meta-programming for Haskell.
Haskell Workshop, Oct. 2002, Pittsburgh

\bibitem {Lisp:Macros}
Steele, Jr., Guy, L.
Common Lisp, the Language.
Digital Press, second edition (1990)

\bibitem {Scheme:Macros}
Clinger, William, Rees, Jonathan et al.
The revised report on the algorithmic language Scheme.
LISP Pointers, 4, 3 (1991)

\bibitem {Scheme:Compilable}
Flatt, M.:
Composable and Compilable Macros.
ICFP, Oct. 2002, Pittsburgh

\bibitem {Meta:Accomplishments}
Sheard, T.:
Accomplishments and Research Challenges in Meta-Programming
2001

\bibitem {Macros:FirstClass}
Bawden, A.:
First-class Macros Have Types
Symposium on Principles of Programming Languages, 2000

\bibitem {AspectJ}
AspectJ

\end{thebibliography}

\end{document}
